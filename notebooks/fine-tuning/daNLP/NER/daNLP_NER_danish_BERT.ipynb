{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook implementing Danish BERT for NER classification on the DaNE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: danlp in /opt/conda/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: conllu in /opt/conda/lib/python3.9/site-packages (from danlp) (4.5.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from danlp) (4.62.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from danlp) (1.4.3)\n",
      "Requirement already satisfied: pyconll in /opt/conda/lib/python3.9/site-packages (from danlp) (3.1.0)\n",
      "Requirement already satisfied: progressbar in /opt/conda/lib/python3.9/site-packages (from danlp) (2.5)\n",
      "Requirement already satisfied: tweepy in /opt/conda/lib/python3.9/site-packages (from danlp) (4.10.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.9/site-packages (from pandas->danlp) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->danlp) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->danlp) (2021.3)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from tweepy->danlp) (1.3.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from tweepy->danlp) (3.2.0)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /opt/conda/lib/python3.9/site-packages (from tweepy->danlp) (2.27.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->danlp) (1.15.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy->danlp) (2.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy->danlp) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy->danlp) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy->danlp) (3.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install danlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading packages\n",
    "## Standard packages\n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## pyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "## Transformers\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from transformers import BertForTokenClassification, AutoModelForTokenClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "## Other ML utils\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from tqdm import tqdm,trange\n",
    "\n",
    "from danlp.datasets import DDT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/danish/-l-ctra/data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"./../../../../data\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and inspecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DaNE data\n",
    "train, val, test = DDT().load_as_simple_ner(predefined_splits=True)\n",
    "\n",
    "# Split sentences and labels\n",
    "tr_sentences, tr_labels = train\n",
    "val_sentences, val_labels = val\n",
    "test_sentences, test_labels = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_misc(ent: str):\n",
    "    if len(ent) < 4:\n",
    "        return False\n",
    "    return ent[-4:] == 'MISC'\n",
    "\n",
    "\n",
    "def remove_miscs(se: list):\n",
    "    return [\n",
    "        [entity if not is_misc(entity) else 'O' for entity in entities]\n",
    "        for entities in se\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace MISC with O for fair comparisons\n",
    "tr_labels = remove_miscs(tr_labels)\n",
    "val_labels = remove_miscs(val_labels)\n",
    "test_labels = remove_miscs(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4383\n",
      "564\n",
      "565\n"
     ]
    }
   ],
   "source": [
    "#Sanity checking number of words\n",
    "print(len(tr_labels))\n",
    "print(len(val_labels))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ELECTRA model requires input data to be in a specific format. One requirement is to have special tokens that marks the beginning ([CLS]) and the separation/end of sentences ([SEP]). These tokens are added to the list of label values below. Furthermore, the label [PAD] is added to indicate padded tokens after padding the sentences later in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', '[PAD]', '[CLS]', '[SEP]']\n",
      "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-LOC': 3, 'I-LOC': 4, 'B-ORG': 5, 'I-ORG': 6, '[PAD]': 7, '[CLS]': 8, '[SEP]': 9}\n",
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-LOC', 4: 'I-LOC', 5: 'B-ORG', 6: 'I-ORG', 7: '[PAD]', 8: '[CLS]', 9: '[SEP]'}\n"
     ]
    }
   ],
   "source": [
    "# Adding labels to fine-tune the BERT\n",
    "tag_values = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG']  #list(set(labels for list in tr_labels for labels in list))\n",
    "tag_values.append(\"[PAD]\")\n",
    "tag_values.append(\"[CLS]\")\n",
    "tag_values.append(\"[SEP]\")\n",
    "print(tag_values)\n",
    "\n",
    "#Creating tag to index and index to tags variables\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "print(tag2idx)\n",
    "print(idx2tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the training data using the vocabulary from multilingual BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A4000'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking whether GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT provides its own tokenizer which is imported below. The tokenizer is created with a Wordpiece model and it creates a vocabulary of whole words, subwords and individual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1656: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer, with manual file address or pretrained address from the Transformers library\n",
    "# where is this file?\n",
    "# https://www.dropbox.com/s/19cjaoqvv2jicq9/danish_bert_uncased_v2.zip?dl=1\n",
    "tokenizer = BertTokenizer.from_pretrained(\"danish_bert_uncased_v2/vocab.txt\", do_lower_case = True, strip_accents = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(s, l)\n",
    "    for s, l in zip(tr_sentences, tr_labels)\n",
    "]\n",
    "\n",
    "val_tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(s, l)\n",
    "    for s, l in zip(val_sentences, val_labels)\n",
    "]\n",
    "\n",
    "test_tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(s, l)\n",
    "    for s, l in zip(test_sentences, test_labels)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['p책',\n",
       "  'fredag',\n",
       "  'har',\n",
       "  'sid',\n",
       "  'inviteret',\n",
       "  'til',\n",
       "  'reception',\n",
       "  'i',\n",
       "  'sid',\n",
       "  '-',\n",
       "  'huset',\n",
       "  'i',\n",
       "  'anledning',\n",
       "  'af',\n",
       "  'at',\n",
       "  'formanden',\n",
       "  'kjeld',\n",
       "  'christensen',\n",
       "  'g책r',\n",
       "  'ind',\n",
       "  'i',\n",
       "  'de',\n",
       "  'glade',\n",
       "  'tre',\n",
       "  '##sser',\n",
       "  '##e',\n",
       "  '.'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-ORG',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'B-LOC',\n",
       "  'B-LOC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-PER',\n",
       "  'I-PER',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_tokenized_texts_and_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'p책', 'fredag', 'har', 'sid', 'inviteret', 'til', 'reception', 'i', 'sid', '-', 'huset', 'i', 'anledning', 'af', 'at', 'formanden', 'kjeld', 'christensen', 'g책r', 'ind', 'i', 'de', 'glade', 'tre', '##sser', '##e', '.', '[SEP]']\n",
      "['[CLS]', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tr_tokenized_texts = [[\"[CLS]\"] + tr_token_label_pair[0] + [\"[SEP]\"] for tr_token_label_pair in tr_tokenized_texts_and_labels]\n",
    "tr_labels = [[\"[CLS]\"] + tr_token_label_pair[1] + [\"[SEP]\"] for tr_token_label_pair in tr_tokenized_texts_and_labels]\n",
    "\n",
    "\n",
    "val_tokenized_texts = [[\"[CLS]\"] + val_token_label_pair[0] + [\"[SEP]\"] for val_token_label_pair in val_tokenized_texts_and_labels]\n",
    "val_labels = [[\"[CLS]\"] + val_token_label_pair[1] + [\"[SEP]\"] for val_token_label_pair in val_tokenized_texts_and_labels]\n",
    " \n",
    "\n",
    "test_tokenized_texts = [[\"[CLS]\"] + test_token_label_pair[0] + [\"[SEP]\"] for test_token_label_pair in test_tokenized_texts_and_labels]\n",
    "test_labels = [[\"[CLS]\"] + test_token_label_pair[1] + [\"[SEP]\"] for test_token_label_pair in test_tokenized_texts_and_labels]\n",
    "\n",
    "#Example of word-piece tokenizations:\n",
    "print(tr_tokenized_texts[0])\n",
    "print(tr_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that number of words in all datasets are increased due to the word-piece tokenization. For the test dataset this means that it will have a higher number of words i.e. labels also during evaluation and comparison to the rule-based classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13017"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of words increased from 7,416 to 22,424\n",
    "tmp=0\n",
    "for labels in test_labels:\n",
    "    tmp=tmp+len(labels)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# Len of the sentence must be not bigger than the training model\n",
    "# See model's 'max_position_embeddings' = 512\n",
    "\n",
    "MAX_LEN = 128# len(max(tr_tokenized_texts, key = len))\n",
    "print(MAX_LEN)\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing tokens in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2    58  3314    87   456  7898    45 13801    23   456   105  2486\n",
      "    23  4872    47    39  6123 17059  6244   564   164    23    69  3806\n",
      "   862  7113 31694   771     3     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "tr_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tr_tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "val_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in val_tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "test_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in test_tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "#Example of indexing\n",
    "print(tr_input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 1 2 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n"
     ]
    }
   ],
   "source": [
    "tr_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in tr_labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"[PAD]\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "\n",
    "val_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in val_labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"[PAD]\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "\n",
    "test_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in test_labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"[PAD]\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "#Example of indexing\n",
    "print(tr_tags[10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating attention masks that indicates which elements in the sentence are tokens and which are padding elements. So here we create the mask to ignore the padded elements in the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "tr_attention_masks = [[float(i != 0) for i in ii] for ii in tr_input_ids]\n",
    "\n",
    "\n",
    "val_attention_masks = [[float(i != 0) for i in ii] for ii in val_input_ids]\n",
    "\n",
    "test_attention_masks = [[float(i != 0) for i in ii] for ii in test_input_ids]\n",
    "\n",
    "#Example of attention masks\n",
    "print(tr_attention_masks[10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch requires converting datasets into torch tensors (multidimensional matrices). Inputs, tags and mask ID's for training and test data are converted to tensors and moved to the GPU by applying .to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_input_ids)\n",
    "val_inputs = torch.tensor(val_input_ids)\n",
    "test_inputs = torch.tensor(test_input_ids)\n",
    "\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "test_tags = torch.tensor(test_tags)\n",
    "\n",
    "tr_masks = torch.tensor(tr_attention_masks)\n",
    "val_masks = torch.tensor(val_attention_masks)\n",
    "test_masks = torch.tensor(test_attention_masks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating training and test tensor datasets and defining data loaders. Shuffling the training data with RandomSampler and at test time we just pass them sequentially with the SequentialSampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_tags)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the pre-trained bert-base-cased model and provide the number of possible labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at danish_bert_uncased_v2 were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at danish_bert_uncased_v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pt_model_dir = \"danish_bert_uncased_v2\"\n",
    "\n",
    "# Will load config and weight with from_pretrained(). \n",
    "model = BertForTokenClassification.from_pretrained(pt_model_dir, num_labels=len(tag2idx), output_attentions = False, output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to GPU,if you are using GPU machine\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 110034442\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of trainable parameters: {model.num_parameters()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting full finetuning to true because we have capacity to fine tune all layers / update all weights. Before we can start the fine-tuning process, we have to setup the optimizer and add the parameters it should update. A common choice is the AdamW optimizer. We also add some weight_decay as regularization to the main weight matrices. If you have limited resources, you can also try to just train the linear classifier on top of BERT and keep all other weights fixed. This will still give you a good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_tok = tokenizer.vocab[\"[PAD]\"]\n",
    "sep_tok = tokenizer.vocab[\"[SEP]\"]\n",
    "cls_tok = tokenizer.vocab[\"[CLS]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(valid_tags, pred_tags):\n",
    "\n",
    "    \"\"\"\n",
    "    Define a flat accuracy metric to use while training the model.\n",
    "    \"\"\"\n",
    "\n",
    "    return (np.array(valid_tags) == np.array(pred_tags)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annot_confusion_matrix(valid_tags, pred_tags):\n",
    "\n",
    "    \"\"\"\n",
    "    Create an annotated confusion matrix by adding label\n",
    "    annotations and formatting to sklearn's `confusion_matrix`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create header from unique tags\n",
    "    header = sorted(list(set(valid_tags + pred_tags)))\n",
    "\n",
    "    # Calculate the actual confusion matrix\n",
    "    matrix = confusion_matrix(valid_tags, pred_tags, labels = ['B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG'])\n",
    "\n",
    "    # Final formatting touches for the string output\n",
    "    mat_formatted = [header[i] + \"\\t\" + str(row) for i, row in enumerate(matrix)]\n",
    "    content = \"\\t\" + \" \".join(header) + \"\\n\" + \"\\n\".join(mat_formatted)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,        \n",
    "    lr=3e-5 #The authors of BERT uses 3e-5 as lr for BERT-base\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4 # Train a maximum of 3-4 epochs. More will simply result in overfitting the training data. \n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0,\n",
    "    \n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0% 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training loop.\n",
      "Train loss: 0.18414084083081161\n",
      "Train accuracy: 0.8265796762945476\n",
      "Starting validation loop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch:  25% 1/4 [00:28<01:25, 28.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.052569341328408986\n",
      "\n",
      "Validation Accuracy: 0.9705475370691903\n",
      "\n",
      "F1-Score: 0.7323146576117714\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.63      0.77      0.70       188\n",
      "       B-ORG       0.68      0.51      0.58       197\n",
      "       B-PER       0.85      0.82      0.84       251\n",
      "       I-LOC       0.00      0.00      0.00        21\n",
      "       I-ORG       0.69      0.14      0.23        64\n",
      "       I-PER       0.88      0.93      0.90       201\n",
      "           O       0.99      0.99      0.99     11273\n",
      "       [PAD]       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97     12195\n",
      "   macro avg       0.59      0.52      0.53     12195\n",
      "weighted avg       0.97      0.97      0.97     12195\n",
      "\n",
      "Confusion Matrix:\n",
      " \tB-LOC B-ORG B-PER I-LOC I-ORG I-PER O [PAD]\n",
      "B-LOC\t[207  12  10   0   2   0]\n",
      "B-ORG\t[  9 186   2   0   0   0]\n",
      "B-PER\t[  2   0 145   0   6   0]\n",
      "I-LOC\t[ 0  0 15  0  2  1]\n",
      "I-ORG\t[ 12   0  22   0 100   0]\n",
      "I-PER\t[ 0  7 10  0 19  9]\n",
      "\n",
      "Starting training loop.\n",
      "Train loss: 0.040846605506474086\n",
      "Train accuracy: 0.9761813114757106\n",
      "Starting validation loop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50% 2/4 [00:55<00:55, 27.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.03748501568204827\n",
      "\n",
      "Validation Accuracy: 0.9789213830099768\n",
      "\n",
      "F1-Score: 0.8146157979580871\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.75      0.89      0.81       188\n",
      "       B-ORG       0.73      0.69      0.71       197\n",
      "       B-PER       0.88      0.91      0.90       251\n",
      "       I-LOC       0.50      0.14      0.22        21\n",
      "       I-ORG       0.57      0.62      0.60        64\n",
      "       I-PER       0.93      0.92      0.92       201\n",
      "           O       0.99      0.99      0.99     11273\n",
      "\n",
      "    accuracy                           0.98     12195\n",
      "   macro avg       0.77      0.74      0.74     12195\n",
      "weighted avg       0.98      0.98      0.98     12195\n",
      "\n",
      "Confusion Matrix:\n",
      " \tB-LOC B-ORG B-PER I-LOC I-ORG I-PER O\n",
      "B-LOC\t[229   5   2   0   4   0]\n",
      "B-ORG\t[ 14 184   0   0   2   1]\n",
      "B-PER\t[  0   0 167   1   9   1]\n",
      "I-LOC\t[ 0  0 13  3  1  4]\n",
      "I-ORG\t[  4   0  16   0 135   6]\n",
      "I-PER\t[ 0  1  6  2  7 40]\n",
      "\n",
      "Starting training loop.\n",
      "Train loss: 0.0340237199553173\n",
      "Train accuracy: 0.9869368969935151\n",
      "Starting validation loop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75% 3/4 [01:23<00:27, 27.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.038866938919656806\n",
      "\n",
      "Validation Accuracy: 0.9815739398769429\n",
      "\n",
      "F1-Score: 0.8399781540141997\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.83      0.89      0.86       188\n",
      "       B-ORG       0.78      0.71      0.74       197\n",
      "       B-PER       0.90      0.91      0.91       251\n",
      "       I-LOC       0.62      0.38      0.47        21\n",
      "       I-ORG       0.60      0.67      0.63        64\n",
      "       I-PER       0.95      0.92      0.93       201\n",
      "           O       0.99      0.99      0.99     11273\n",
      "\n",
      "    accuracy                           0.98     12195\n",
      "   macro avg       0.81      0.78      0.79     12195\n",
      "weighted avg       0.98      0.98      0.98     12195\n",
      "\n",
      "Confusion Matrix:\n",
      " \tB-LOC B-ORG B-PER I-LOC I-ORG I-PER O\n",
      "B-LOC\t[228   2   2   0   2   0]\n",
      "B-ORG\t[ 15 184   0   0   0   1]\n",
      "B-PER\t[  0   0 167   1  10   0]\n",
      "I-LOC\t[0 0 7 8 2 3]\n",
      "I-ORG\t[  1   0  11   0 139   2]\n",
      "I-PER\t[ 0  1  5  4  4 43]\n",
      "\n",
      "Starting training loop.\n",
      "Train loss: 0.030628439637213727\n",
      "Train accuracy: 0.9926473968671335\n",
      "Starting validation loop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch: 100% 4/4 [01:50<00:00, 27.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.03695434404330121\n",
      "\n",
      "Validation Accuracy: 0.9826229861430795\n",
      "\n",
      "F1-Score: 0.8502415458937198\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.86      0.88      0.87       188\n",
      "       B-ORG       0.76      0.78      0.77       197\n",
      "       B-PER       0.94      0.90      0.92       251\n",
      "       I-LOC       0.40      0.67      0.50        21\n",
      "       I-ORG       0.61      0.69      0.65        64\n",
      "       I-PER       0.94      0.94      0.94       201\n",
      "           O       0.99      0.99      0.99     11273\n",
      "       [PAD]       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98     12195\n",
      "   macro avg       0.69      0.73      0.71     12195\n",
      "weighted avg       0.98      0.98      0.98     12195\n",
      "\n",
      "Confusion Matrix:\n",
      " \tB-LOC B-ORG B-PER I-LOC I-ORG I-PER O [PAD]\n",
      "B-LOC\t[227   3   2   0   5   0]\n",
      "B-ORG\t[  9 189   0   0   0   3]\n",
      "B-PER\t[  0   0 165   8  11   0]\n",
      "I-LOC\t[ 0  0  2 14  0  5]\n",
      "I-ORG\t[  1   0  10   0 153   1]\n",
      "I-PER\t[ 0  1  0 10  2 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "tr_loss_values, eval_loss_values = [], []\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    epoch += 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"\\nStarting training loop.\")\n",
    "    model.train()\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Add batch to gpu\n",
    "        batch = tuple(t.to(torch.int64).to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels,\n",
    "        )\n",
    "        loss, tr_logits = outputs[:2]\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Compute train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        # Subset out unwanted predictions on CLS/PAD/SEP tokens\n",
    "        preds_mask = (\n",
    "            (b_input_ids != cls_tok)\n",
    "            & (b_input_ids != pad_tok)\n",
    "            & (b_input_ids != sep_tok)\n",
    "        )\n",
    "\n",
    "        #preds_mask = preds_mask.detach().cpu().numpy()\n",
    "        tr_logits = tr_logits.detach().cpu().numpy()\n",
    "        tr_label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
    "        tr_batch_preds = np.argmax(tr_logits[preds_mask.detach().cpu().numpy().squeeze()], axis=1)\n",
    "        tr_batch_labels = tr_label_ids.to(\"cpu\").numpy()\n",
    "        tr_preds.extend(tr_batch_preds)\n",
    "        tr_labels.extend(tr_batch_labels)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        tmp_tr_accuracy = flat_accuracy(tr_batch_labels, tr_batch_preds)\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=max_grad_norm\n",
    "        )\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "    tr_loss = tr_loss / nb_tr_steps\n",
    "    tr_loss_values.append(tr_loss)\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "\n",
    "    # Print training loss and accuracy per epoch\n",
    "    print(f\"Train loss: {tr_loss}\")\n",
    "    print(f\"Train accuracy: {tr_accuracy}\")\n",
    "\n",
    "    # Validation loop\n",
    "    print(\"Starting validation loop.\")\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    for batch in valid_dataloader:\n",
    "\n",
    "        batch = tuple(t.to(torch.int64).to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "        # Subset out unwanted predictions on CLS/PAD/SEP tokens\n",
    "        preds_mask = (\n",
    "            (b_input_ids != cls_tok)\n",
    "            & (b_input_ids != pad_tok)\n",
    "            & (b_input_ids != sep_tok)\n",
    "        )\n",
    "\n",
    "        logits = logits.to(\"cpu\").numpy()\n",
    "        label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
    "        val_batch_preds = np.argmax(logits[preds_mask.detach().cpu().numpy().squeeze()], axis=1)\n",
    "        val_batch_labels = label_ids.to(\"cpu\").numpy()\n",
    "        predictions.extend(val_batch_preds)\n",
    "        true_labels.extend(val_batch_labels)\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(val_batch_labels, val_batch_preds)\n",
    "\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "\n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Evaluate loss, acc, conf. matrix, and class. report on validation set\n",
    "    pred_tags = [idx2tag[i] for i in predictions]\n",
    "    valid_tags = [idx2tag[i] for i in true_labels]\n",
    "    cl_report = classification_report(valid_tags, pred_tags)\n",
    "    conf_mat = annot_confusion_matrix(valid_tags, pred_tags)\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_loss_values.append(eval_loss)\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    f1score = f1_score(valid_tags, pred_tags, labels = ['B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG'], average=\"micro\")\n",
    "\n",
    "    # Report metrics\n",
    "    print(f\"Validation loss: {eval_loss}\\n\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\\n\")\n",
    "    print(f\"F1-Score: {f1score}\\n\")\n",
    "    print(f\"Classification Report:\\n {cl_report}\")\n",
    "    print(f\"Confusion Matrix:\\n {conf_mat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(tr_loss_values, 'b-o', label=\"training loss\")\n",
    "plt.plot(eval_loss_values, 'r-o', label=\"validation loss\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Learning curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model_path = 'research/daNLP/NER/models/Danish_BERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dir if not exits\n",
    "if not os.path.exists(ner_model_path):\n",
    "        os.makedirs(ner_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('research/daNLP/NER/models/Danish_BERT/tokenizer_config.json',\n",
       " 'research/daNLP/NER/models/Danish_BERT/special_tokens_map.json',\n",
       " 'research/daNLP/NER/models/Danish_BERT/vocab.txt',\n",
       " 'research/daNLP/NER/models/Danish_BERT/added_tokens.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model and the tokenizer\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model #Take care of distrubuted/parallel training\n",
    "model_to_save.save_pretrained(ner_model_path)\n",
    "tokenizer.save_pretrained(ner_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model to tokenize the test sentece\n",
    "model = BertForTokenClassification.from_pretrained(ner_model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(ner_model_path, do_lower_case = True, strip_accents = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Epochs: 4\n",
      "\n",
      "Test loss: 0.041447424019376435\n",
      "\n",
      "Test Accuracy: 0.9773173648600353\n",
      "\n",
      "F1-Score Micro: 0.8301382077574676\n",
      "\n",
      "F1-Score Macro: 0.738112888238303\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.81      0.87      0.83       172\n",
      "       B-PER       0.87      0.91      0.89       274\n",
      "       I-LOC       0.21      0.88      0.34         8\n",
      "       I-PER       0.96      1.00      0.98       260\n",
      "       B-ORG       0.81      0.64      0.71       333\n",
      "       I-ORG       0.79      0.57      0.67        94\n",
      "\n",
      "   micro avg       0.84      0.82      0.83      1141\n",
      "   macro avg       0.74      0.81      0.74      1141\n",
      "weighted avg       0.85      0.82      0.83      1141\n",
      "\n",
      "Confusion Matrix:\n",
      " \tB-LOC B-ORG B-PER I-LOC I-ORG I-PER O\n",
      "B-LOC\t[249   3   0   0   2   0]\n",
      "B-ORG\t[  0 260   0   0   0   0]\n",
      "B-PER\t[  0   0 149   4   1   0]\n",
      "I-LOC\t[0 0 1 7 0 0]\n",
      "I-ORG\t[ 30   0  28   4 212   4]\n",
      "I-PER\t[ 0  6  1 16  6 54]\n"
     ]
    }
   ],
   "source": [
    "pad_tok = tokenizer.vocab[\"[PAD]\"]\n",
    "sep_tok = tokenizer.vocab[\"[SEP]\"]\n",
    "cls_tok = tokenizer.vocab[\"[CLS]\"]\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "predictions, true_labels = [], []\n",
    "tr_loss_values, test_loss_values = [], []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "\n",
    "    batch = tuple(t.to(torch.int64).to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels,\n",
    "        )\n",
    "        tmp_test_loss, logits = outputs[:2]\n",
    "\n",
    "    # Subset out unwanted predictions on CLS/PAD/SEP tokens\n",
    "    preds_mask = (\n",
    "        (b_input_ids != cls_tok)\n",
    "        & (b_input_ids != pad_tok)\n",
    "        & (b_input_ids != sep_tok)\n",
    "    )\n",
    "\n",
    "    logits = logits.to(\"cpu\").numpy()\n",
    "    label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
    "    test_batch_preds = np.argmax(logits[preds_mask.detach().cpu().numpy().squeeze()], axis=1)\n",
    "    test_batch_labels = label_ids.to(\"cpu\").numpy()\n",
    "    predictions.extend(test_batch_preds)\n",
    "    true_labels.extend(test_batch_labels)\n",
    "\n",
    "    tmp_test_accuracy = flat_accuracy(test_batch_labels, test_batch_preds)\n",
    "\n",
    "    test_loss += tmp_test_loss.mean().item()\n",
    "    test_accuracy += tmp_test_accuracy\n",
    "\n",
    "\n",
    "    nb_test_examples += b_input_ids.size(0)\n",
    "    nb_test_steps += 1\n",
    "\n",
    "# Evaluate loss, acc, conf. matrix, and class. report on devset\n",
    "pred_tags = [idx2tag[i] for i in predictions]\n",
    "valid_tags = [idx2tag[i] for i in true_labels]\n",
    "cl_report = classification_report(valid_tags, pred_tags, labels = ['B-LOC', 'B-PER', 'I-LOC', 'I-PER', 'B-ORG', 'I-ORG'])\n",
    "conf_mat = annot_confusion_matrix(valid_tags, pred_tags)\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_loss_values.append(test_loss)\n",
    "test_accuracy = test_accuracy / nb_test_steps\n",
    "f1score_micro = f1_score(valid_tags, pred_tags, labels = ['B-LOC', 'B-PER', 'I-LOC', 'I-PER', 'B-ORG', 'I-ORG'], average=\"micro\")\n",
    "f1score_macro = f1_score(valid_tags, pred_tags, labels = ['B-LOC', 'B-PER', 'I-LOC', 'I-PER', 'B-ORG', 'I-ORG'], average=\"macro\")\n",
    "\n",
    "\n",
    "# Report metrics\n",
    "print(f\"Number of Epochs: {epochs}\\n\")\n",
    "\n",
    "print(f\"Test loss: {test_loss}\\n\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\\n\")\n",
    "\n",
    "print(f\"F1-Score Micro: {f1score_micro}\\n\")\n",
    "print(f\"F1-Score Macro: {f1score_macro}\\n\")\n",
    "\n",
    "print(f\"Classification Report:\\n {cl_report}\")\n",
    "print(f\"Confusion Matrix:\\n {conf_mat}\")\n",
    "\n",
    "with open(f'{ner_model_path}/TESTMETRICS','a+') as f:\n",
    "    f.write(f\"Number of Epochs: {epochs}\\n\")\n",
    "\n",
    "    f.write(f\"Test loss: {test_loss}\\n\")\n",
    "    f.write(f\"Test Accuracy: {test_accuracy}\\n\")\n",
    "\n",
    "    f.write(f\"F1-Score Micro: {f1score_micro}\\n\")\n",
    "    f.write(f\"F1-Score Macro: {f1score_macro}\\n\")\n",
    "\n",
    "    f.write(f\"Classification Report:\\n {cl_report}\")\n",
    "    f.write(f\"Confusion Matrix:\\n {conf_mat}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model's capabilities on specific tokens only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test[\"labels\"][test[\"labels\"].str.contains(\"PER\")]=\"B-PER\"\n",
    "valid_tags = pd.Series(valid_tags)\n",
    "valid_tags[valid_tags.str.contains(\"PER\")] = \"B-PER\"\n",
    "valid_tags[valid_tags.str.contains(\"LOC\")] = \"B-LOC\"\n",
    "valid_tags = valid_tags.tolist()\n",
    "\n",
    "pred_tags = pd.Series(pred_tags)\n",
    "pred_tags[pred_tags.str.contains(\"PER\")] = \"B-PER\"\n",
    "pred_tags[pred_tags.str.contains(\"LOC\")] = \"B-LOC\"\n",
    "pred_tags = pred_tags.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.8746787195850696\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.74      0.89      0.81       180\n",
      "       B-PER       0.92      0.96      0.94       534\n",
      "\n",
      "   micro avg       0.87      0.94      0.91       714\n",
      "   macro avg       0.83      0.93      0.87       714\n",
      "weighted avg       0.88      0.94      0.91       714\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  161     1     0     0    18]\n",
      " [   32   212    30     4    55]\n",
      " [    0     2   512     0    20]\n",
      " [   17     6     6    54    11]\n",
      " [    8    40     7    10 10681]]\n"
     ]
    }
   ],
   "source": [
    "cl_report = classification_report(valid_tags, pred_tags, labels = ['B-LOC', 'B-PER'])\n",
    "conf_mat = confusion_matrix(valid_tags, pred_tags)\n",
    "f1score = f1_score(valid_tags, pred_tags, labels = ['B-LOC', 'B-PER'], average = \"macro\")\n",
    "\n",
    "# Report metrics\n",
    "print(f\"F1-Score: {f1score}\\n\")\n",
    "print(f\"Classification Report:\\n {cl_report}\")\n",
    "print(f\"Confusion Matrix:\\n {conf_mat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"Indk첩b af Melon 1 kg, 2 slags Karen Volf 200g, 2 poser Chili og Timian fra Santa Maria, Arla 1L. Vores referencer: Karen Volf, Chili Jensen, Timian Hansen og Arla Kristoffersen. Kontaktperson er Melon Andersen. Levering til Timianvej 12\"\n",
    "test_sentence2 = \"Timian Nielsen har bestilt 10 kasser Lego til levering p책 Hc. Andersensvej 13 A f첩rste Sal tv og han har k첩bt det til sin datter chili som g책r med ben ble fra Abena og hun elsker i 첩vrigt elsker at spise chili, s책 derfor har de 10 kg chili derhjemme, men hvad chili ikke ved er at hendes far har k첩bt en hvid 3 hjulet cykel fra Toys R Us ved Toppen Nr. 3 Aarhus- helt specifikt er det en 3 hjulet nr 30 fra kataloget og han har husket Toppen beskyttelseshjelm, str 35 og Far Timian kan godt lide chiLi men han elsker at spise en Tivoli stang, derfor bestilte han 20 stk Toms tivoli stang s책 han kan dele med sin ven Sebsatian i stedet for at f책 Melon i Gr첩n Box.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3509, 47, 467, 53, 127, 3219, 911, 148, 2009, 8385, 1974, 31708, 808, 31703, 911, 148, 16894, 12390, 28, 28254, 145, 18482, 6357, 911, 26370, 127, 31702, 771, 339, 14209, 1325, 8385, 1974, 31708, 911, 12390, 3683, 911, 28254, 3744, 28, 26370, 26460, 134, 771, 12841, 33, 467, 53, 4459, 771, 4457, 45, 28254, 578, 1082, 3]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([tokenized_sentence]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(input_ids)\n",
    "logits = F.softmax(logits[0], dim = 2)\n",
    "logits_label = torch.argmax(logits, dim = 2)\n",
    "logits_label = logits_label.detach().cpu().numpy().tolist()[0]\n",
    "\n",
    "logits_confidence = [values[label].item() for values, label in zip(logits[0], logits_label)]\n",
    "len(logits_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join bpe split tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "new_tokens, new_labels, new_probs = [], [], []\n",
    "for token, label_idx, probs in zip(tokens, logits_label, logits_confidence):\n",
    "    if token.startswith(\"##\"):\n",
    "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "    else:\n",
    "        new_labels.append(tag_values[label_idx])\n",
    "        new_tokens.append(token)\n",
    "        new_probs.append(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEP]\t[CLS]\t0.35406693816185\n",
      "O\tindk첩b\t0.9997159838676453\n",
      "O\taf\t0.9997386336326599\n",
      "O\tmelon\t0.7387145757675171\n",
      "O\t1\t0.9984843134880066\n",
      "O\tkg\t0.9990200996398926\n",
      "O\t,\t0.9997584223747253\n",
      "O\t2\t0.9996015429496765\n",
      "O\tslags\t0.9996067881584167\n",
      "B-PER\tkaren\t0.9913374185562134\n",
      "I-PER\tvolf\t0.9940366744995117\n",
      "O\t200g\t0.9955313801765442\n",
      "O\t,\t0.9997616410255432\n",
      "O\t2\t0.9997190833091736\n",
      "O\tposer\t0.9996324777603149\n",
      "O\tchili\t0.8727623820304871\n",
      "O\tog\t0.9984689354896545\n",
      "O\ttimian\t0.9261709451675415\n",
      "O\tfra\t0.9994376301765442\n",
      "B-ORG\tsanta\t0.6258220672607422\n",
      "I-ORG\tmaria\t0.49883905053138733\n",
      "O\t,\t0.9965651631355286\n",
      "B-ORG\tarla\t0.5763012766838074\n",
      "I-LOC\t1l\t0.5286656022071838\n",
      "O\t.\t0.999380350112915\n",
      "O\tvores\t0.999336302280426\n",
      "O\treferencer\t0.9991549253463745\n",
      "O\t:\t0.9996308088302612\n",
      "B-PER\tkaren\t0.9944873452186584\n",
      "I-PER\tvolf\t0.9955767393112183\n",
      "O\t,\t0.99953293800354\n",
      "B-PER\tchili\t0.994709849357605\n",
      "I-PER\tjensen\t0.9945797920227051\n",
      "O\t,\t0.999550998210907\n",
      "B-PER\ttimian\t0.992206871509552\n",
      "I-PER\thansen\t0.9924864768981934\n",
      "O\tog\t0.9996300935745239\n",
      "B-PER\tarla\t0.9769644141197205\n",
      "I-PER\tkristoffersen\t0.9837559461593628\n",
      "O\t.\t0.9996211528778076\n",
      "O\tkontaktperson\t0.9986716508865356\n",
      "O\ter\t0.9995716214179993\n",
      "B-PER\tmelon\t0.9917009472846985\n",
      "I-PER\tandersen\t0.9950199127197266\n",
      "O\t.\t0.9996395111083984\n",
      "O\tlevering\t0.9984989166259766\n",
      "O\ttil\t0.9979198575019836\n",
      "B-LOC\ttimianvej\t0.9144138693809509\n",
      "I-LOC\t12\t0.7498582601547241\n",
      "[SEP]\t[SEP]\t0.35403603315353394\n"
     ]
    }
   ],
   "source": [
    "for token, label, prob in zip(new_tokens, new_labels, new_probs):\n",
    "    print(\"{}\\t{}\\t{}\".format(label, token, prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Word': '[CLS]', 'Label': '[SEP]', 'Confidence': 0.35406693816185},\n",
       " {'Word': 'indk첩b', 'Label': 'O', 'Confidence': 0.9997159838676453},\n",
       " {'Word': 'af', 'Label': 'O', 'Confidence': 0.9997386336326599},\n",
       " {'Word': 'melon', 'Label': 'O', 'Confidence': 0.7387145757675171},\n",
       " {'Word': '1', 'Label': 'O', 'Confidence': 0.9984843134880066},\n",
       " {'Word': 'kg', 'Label': 'O', 'Confidence': 0.9990200996398926},\n",
       " {'Word': ',', 'Label': 'O', 'Confidence': 0.9997584223747253},\n",
       " {'Word': '2', 'Label': 'O', 'Confidence': 0.9996015429496765},\n",
       " {'Word': 'slags', 'Label': 'O', 'Confidence': 0.9996067881584167},\n",
       " {'Word': 'karen', 'Label': 'B-PER', 'Confidence': 0.9913374185562134},\n",
       " {'Word': 'volf', 'Label': 'I-PER', 'Confidence': 0.9940366744995117},\n",
       " {'Word': '200g', 'Label': 'O', 'Confidence': 0.9955313801765442},\n",
       " {'Word': ',', 'Label': 'O', 'Confidence': 0.9997616410255432},\n",
       " {'Word': '2', 'Label': 'O', 'Confidence': 0.9997190833091736},\n",
       " {'Word': 'poser', 'Label': 'O', 'Confidence': 0.9996324777603149},\n",
       " {'Word': 'chili', 'Label': 'O', 'Confidence': 0.8727623820304871},\n",
       " {'Word': 'og', 'Label': 'O', 'Confidence': 0.9984689354896545},\n",
       " {'Word': 'timian', 'Label': 'O', 'Confidence': 0.9261709451675415},\n",
       " {'Word': 'fra', 'Label': 'O', 'Confidence': 0.9994376301765442},\n",
       " {'Word': 'santa', 'Label': 'B-ORG', 'Confidence': 0.6258220672607422},\n",
       " {'Word': 'maria', 'Label': 'I-ORG', 'Confidence': 0.49883905053138733},\n",
       " {'Word': ',', 'Label': 'O', 'Confidence': 0.9965651631355286},\n",
       " {'Word': 'arla', 'Label': 'B-ORG', 'Confidence': 0.5763012766838074},\n",
       " {'Word': '1l', 'Label': 'I-LOC', 'Confidence': 0.5286656022071838},\n",
       " {'Word': '.', 'Label': 'O', 'Confidence': 0.999380350112915},\n",
       " {'Word': 'vores', 'Label': 'O', 'Confidence': 0.999336302280426},\n",
       " {'Word': 'referencer', 'Label': 'O', 'Confidence': 0.9991549253463745},\n",
       " {'Word': ':', 'Label': 'O', 'Confidence': 0.9996308088302612},\n",
       " {'Word': 'karen', 'Label': 'B-PER', 'Confidence': 0.9944873452186584},\n",
       " {'Word': 'volf', 'Label': 'I-PER', 'Confidence': 0.9955767393112183},\n",
       " {'Word': ',', 'Label': 'O', 'Confidence': 0.99953293800354},\n",
       " {'Word': 'chili', 'Label': 'B-PER', 'Confidence': 0.994709849357605},\n",
       " {'Word': 'jensen', 'Label': 'I-PER', 'Confidence': 0.9945797920227051},\n",
       " {'Word': ',', 'Label': 'O', 'Confidence': 0.999550998210907},\n",
       " {'Word': 'timian', 'Label': 'B-PER', 'Confidence': 0.992206871509552},\n",
       " {'Word': 'hansen', 'Label': 'I-PER', 'Confidence': 0.9924864768981934},\n",
       " {'Word': 'og', 'Label': 'O', 'Confidence': 0.9996300935745239},\n",
       " {'Word': 'arla', 'Label': 'B-PER', 'Confidence': 0.9769644141197205},\n",
       " {'Word': 'kristoffersen', 'Label': 'I-PER', 'Confidence': 0.9837559461593628},\n",
       " {'Word': '.', 'Label': 'O', 'Confidence': 0.9996211528778076},\n",
       " {'Word': 'kontaktperson', 'Label': 'O', 'Confidence': 0.9986716508865356},\n",
       " {'Word': 'er', 'Label': 'O', 'Confidence': 0.9995716214179993},\n",
       " {'Word': 'melon', 'Label': 'B-PER', 'Confidence': 0.9917009472846985},\n",
       " {'Word': 'andersen', 'Label': 'I-PER', 'Confidence': 0.9950199127197266},\n",
       " {'Word': '.', 'Label': 'O', 'Confidence': 0.9996395111083984},\n",
       " {'Word': 'levering', 'Label': 'O', 'Confidence': 0.9984989166259766},\n",
       " {'Word': 'til', 'Label': 'O', 'Confidence': 0.9979198575019836},\n",
       " {'Word': 'timianvej', 'Label': 'B-LOC', 'Confidence': 0.9144138693809509},\n",
       " {'Word': '12', 'Label': 'I-LOC', 'Confidence': 0.7498582601547241},\n",
       " {'Word': '[SEP]', 'Label': '[SEP]', 'Confidence': 0.35403603315353394}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_predictions = [{\"Word\":token,\"Label\":label,\"Confidence\":prob} for token, label, prob in zip(new_tokens, new_labels, new_probs)]\n",
    "dict_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
